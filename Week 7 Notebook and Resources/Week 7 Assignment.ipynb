{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "\n",
    "This week you will begin a final project in this course broken up over different assignments in the last three weeks. Choose a machine learning appropriate dataset and import it into Python using Pandas or NumPy. If you are using a classification dataset, your target must be one-hot encoded. You will need to import the dataset so it is compatible with Scikit-learn/Keras, and it may not be one we have used previously in class. Finish this assignment by splitting the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research, but in order to do that, there is a critical condition to be sorted out, findinfinding the right dataset to test and build predictive models.\n",
    "\n",
    "That is a key aspect and a common problem in Machine Learning. Fortunately, the <a href=\"https://keras.io/datasets/\">Keras.Datasets</a> module already includes methods to load and fetch popular reference datasets. Here's the list of available datasets:\n",
    "\n",
    "<ul>\n",
    "  <li><b>boston_housing module:</b> Boston housing price regression dataset.</li>\n",
    "  <li><b>cifar10 module:</b> CIFAR10 small images classification dataset (classification of 10 image labels).</li>\n",
    "  <li><b>cifar100 module:</b> CIFAR100 small images classification dataset (classification of 100 image labels).</li>\n",
    "  <li><b>fashion_mnist module:</b> Fashion-MNIST dataset (classification of 10 digits).</li>\n",
    "  <li><b>imdb module:</b> IMDB sentiment classification dataset (classification of 10 fashion categories).</li>\n",
    "  <li><b>mnist module:</b> MNIST handwritten digits dataset (binary text classification).</li>\n",
    "  <li><b>reuters module:</b> Reuters topic classification dataset(multiclass text classification).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "\n",
    "For our purposes, and considering the considerations made on the assignment description, I have choose the CIRFAR10 as Dataset for the upcomming project.\n",
    "\n",
    "<b><ins>CIFAR10:</ins></b> The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. The dataset information and details can be found <a href=\"https://www.cs.toronto.edu/~kriz/cifar.html\">here</a>.\n",
    "\n",
    "he CIFAR-10 and CIFAR-100 are labeled subsets of the 80 million tiny images dataset. They were collected by Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.The tech report <a href=\"https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf\">Learning Multiple Layers of Features from Tiny Images, 2009</a> on its Chapter 3, describes the dataset and the methodology followed when collecting it in much greater detail.\n",
    "\n",
    "Considering that the scope of the present assignment is to load the dataset validating its format and split it into training and testing groups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6 - Step by Step\n",
    "\n",
    "The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. Even that is not going to be used at this time, it is convenient to consider it since is going to be a vital part of our project.\n",
    "\n",
    "Also, as described above, we will be using the build-in dataset CIFAR10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from termcolor import colored\n",
    "\n",
    "# Showing the shape of the training and testing datasets\n",
    "def show_shapes(x_train, y_train, x_test, y_test, color='blue'):\n",
    "    print(colored('Training:', color, attrs=['bold']))\n",
    "    print('  x_train.shape:', x_train.shape)\n",
    "    print('  y_train.shape:', y_train.shape)\n",
    "    print(colored('\\nTesting:', color, attrs=['bold']))\n",
    "    print('  x_test.shape:', x_test.shape)\n",
    "    print('  y_test.shape:', y_test.shape)\n",
    "    \n",
    "def plot_data(my_data, cmap=None):\n",
    "    plt.axis('off')\n",
    "    fig = plt.imshow(my_data, cmap=cmap)\n",
    "    fig.axes.get_xaxis().set_visible(False)\n",
    "    fig.axes.get_yaxis().set_visible(False)\n",
    "    print(fig)\n",
    "\n",
    "# Showing the x,y array for a sample image    \n",
    "def show_sample(x_train, y_train, idx=0, color='blue'):\n",
    "    print(colored('x_train sample:', color, attrs=['bold']))\n",
    "    print(x_train[idx])\n",
    "    print(colored('\\ny_train sample:', color, attrs=['bold']))\n",
    "    print(y_train[idx])\n",
    "\n",
    "    # Showing the image from the x,y array sample previously given    \n",
    "def show_sample_image(x_train, y_train, idx=0, color='blue', cmap=None):\n",
    "    print(colored('Label:', color, attrs=['bold']), y_train[idx])\n",
    "    print(colored('Shape:', color, attrs=['bold']), x_train[idx].shape)\n",
    "    print()\n",
    "    plot_data(x_train[idx], cmap=cmap)\n",
    "\n",
    "# Declare variables\n",
    "num_classes = 10\n",
    "class_label=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','trunk']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to load CIFAR10 dataset into our model, obtaining two tuples of Numpy arrays: \n",
    "\n",
    "<b><i> (x_train, y_train), (x_test, y_test). </i></b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "show_shapes(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(x_train, y_train)\n",
    "print '\\n' + 30*'-' + '\\n'\n",
    "print '\\nImage index 0\\n'\n",
    "show_sample_image(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_sample(x_train, y_train,100)\n",
    "print '\\n' + 30*'-' + '\\n'\n",
    "print '\\nImage index 1000\\n'\n",
    "show_sample_image(x_train, y_train,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print figure with 10 random images from cifar dataset\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "for i in range(num_classes):\n",
    "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
    "    idx = np.where(y_train[:]==i)[0]\n",
    "    features_idx = x_train[idx,::]\n",
    "    img_num = np.random.randint(features_idx.shape[0])\n",
    "    im = np.transpose(features_idx[img_num,::],(0,1,2))\n",
    "    ax.set_title(class_label[i])\n",
    "    plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 6 Conclusion\n",
    "We understand that loading the CIFAR10 dataset we obtain:\n",
    "\n",
    "<ul>\n",
    "  <li><b><i>x_train and x_test</b></i>\n",
    "    <ul>\n",
    "      <li>uint8 array of RGB image data with shape (num_samples, 32, 32, 3).</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "  <li><b><i>y_train and y_test</b></i>\n",
    "    <ul>\n",
    "      <li>uint8 array of category labels (integers in range 0-9) with shape (num_samples, 1).</li>\n",
    "    </ul>\n",
    "  </li>\n",
    "</ul>\n",
    "\n",
    "We can see also that the pixel values are in the range of 0 to 255 for each of the red, green and blue channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 7 - Step by Step\n",
    "Now, we need to convert the label class (Y axis) to ensure the proper manipulation of the data by the model. This can be achieved by converting a class vector (integers) to binary class matrix with the <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical\">to_categorical</a> method defined on keras.\n",
    "\n",
    "Besides that, it’s good practice to work with normalized data.\n",
    "\n",
    "Because the input values are well understood, we can easily normalize to the range 0 to 1 by dividing each value by the maximum observation which is 255.\n",
    "\n",
    "Note, the data is loaded as integers, so we must cast it to floating point values in order to perform the division.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train  /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s start by defining a simple CNN model. We will use a model with one convolutional layers followed by max pooling and a flattening out of the network to fully connected layers to make predictions.\n",
    "\n",
    "We will be using Droupout as regularization technique, implementing it in at two points on our model, the first one right before flattening our data out with 25% and the second one right before the output layer with 50%.\n",
    "\n",
    "As activation function, we are considering ReLU on every layer, except for the output one, using softmax for multi-dimentional evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:], activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logarithmic loss function is used with the stochastic gradient descent optimization algorithm configured with a large momentum and weight decay start with a learning rate of 0.1.\n",
    "We are using \"categorical_crossentropy\" for loss with metric \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have the following model created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of neural network\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "# Output network visualization\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fit this model with 100 epochs and a batch size of 32. We choose the batch size of 32 examples as a mini-batch, smaller batch size means more updates in one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "# 32 examples in a mini-batch, smaller batch size means more updates in one epoch\n",
    "epochs = 20 # repeat 100 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to an utility embeded in Keras for Image Processing called <a href=\"https://keras.io/preprocessing/image/\">ImageDataGenerator</a> which generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1)\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets fit the model on the batches generated by datagen.flow()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import livelossplot\n",
    "plot_losses = livelossplot.PlotLossesKeras()\n",
    "\n",
    "model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                        batch_size=batch_size),\n",
    "                        steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4,\n",
    "                        callbacks=[plot_losses])\n",
    "\n",
    "# Loss and Accuracy\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
